{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101f30a0-63b2-4e91-918d-b38cd862ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PERCY JACKSON\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PERCY JACKSON\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import defaultdict\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO(\"C:/Users/PERCY JACKSON/OneDrive/Desktop/best.pt\")\n",
    "\n",
    "# Load InceptionResNetV2 classification model\n",
    "inception_model = load_model(\"C:/Users/PERCY JACKSON/OneDrive/Desktop/Restnetv2_model.keras\")\n",
    "\n",
    "# Class ID to Name mapping\n",
    "class_names = {\n",
    "    0: 'Cardboard',\n",
    "    1: 'Glass',\n",
    "    2: 'Metal',\n",
    "    3: 'Paper',\n",
    "    4: 'Plastic',\n",
    "    5: 'Trash'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78ae53a-8903-4bdf-a208-45686cc9ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    # Save temporarily\n",
    "    temp_path = \"C:/Users/PERCY JACKSON/OneDrive/Desktop/test2.jpg\"\n",
    "    cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Step 1: Detection\n",
    "    results = yolo_model(temp_path, conf=0.5, iou=0.3)\n",
    "    detected_boxes = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            confidence = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            detected_boxes.append((x1, y1, x2, y2, confidence, class_id))\n",
    "\n",
    "    # Step 2: Cropping and preprocessing\n",
    "    cropped_images = []\n",
    "    img = cv2.imread(temp_path)\n",
    "    for (x1, y1, x2, y2, confidence, class_id) in detected_boxes:\n",
    "        cropped_img = img[y1:y2, x1:x2]\n",
    "        if cropped_img.size == 0:\n",
    "            continue\n",
    "        resized_img = cv2.resize(cropped_img, (224, 224)) / 255.0\n",
    "        cropped_images.append((resized_img, (x1, y1, x2, y2, class_id)))\n",
    "\n",
    "    # Step 3: Classify with Inception model\n",
    "    corrected_labels = []\n",
    "    for img, (x1, y1, x2, y2, class_id) in cropped_images:\n",
    "        img_input = np.expand_dims(img, axis=0)\n",
    "        prediction = inception_model.predict(img_input, verbose=0)\n",
    "        corrected_class_id = np.argmax(prediction)\n",
    "        corrected_labels.append((x1, y1, x2, y2, corrected_class_id))\n",
    "\n",
    "    # Step 4: Draw final results\n",
    "    final_img = cv2.imread(temp_path)\n",
    "    final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "    for (x1, y1, x2, y2, class_id) in corrected_labels:\n",
    "        class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
    "        cv2.rectangle(final_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(final_img, f\"{class_name}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    # Step 5: Percentage calculation\n",
    "    class_areas = defaultdict(float)\n",
    "    total_area = 0.0\n",
    "    for (x1, y1, x2, y2, class_id) in corrected_labels:\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "        class_areas[class_id] += area\n",
    "        total_area += area\n",
    "\n",
    "    percentages = {class_names[class_id]: round((area / total_area) * 100, 2)\n",
    "                   for class_id, area in class_areas.items()}\n",
    "\n",
    "    # Step 6: Generate label output\n",
    "    percentage_str = \"\\n\".join([f\"{k}: {v}%\" for k, v in percentages.items()])\n",
    "\n",
    "    return final_img, percentage_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa8b524-292a-4baa-8f65-261f0c614ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\PERCY JACKSON\\OneDrive\\Desktop\\test2.jpg: 640x480 1 Cardboard, 1 Glass, 2 Metals, 1 Paper, 1 Trash, 352.1ms\n",
      "Speed: 16.0ms preprocess, 352.1ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload Waste Image\"),\n",
    "    outputs=[\n",
    "        gr.Image(type=\"numpy\", label=\"Detected Output\"),\n",
    "        gr.Textbox(label=\"Class Percentage Composition\")\n",
    "    ],\n",
    "    title=\"Smart Waste Classifier\",\n",
    "    description=\"Upload an image to detect and classify waste items using YOLOv8 + InceptionResNetV2 pipeline.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c9554-1f1c-4cf2-a920-c51c6547d1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
